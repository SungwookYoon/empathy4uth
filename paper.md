# 가스라이팅 탐지 및 개입 시스템 개발 보고서

## 1. 연구 개요

본 연구는 청소년 디지털 의사소통에서 발생하는 가스라이팅을 탐지하고 적절한 개입을 제공하는 AI 시스템을 개발하는 것을 목표로 합니다. 가스라이팅은 타인의 현실 인식을 왜곡하고 자신의 감정과 경험을 의심하게 만드는 심리적 조작 형태로, 청소년의 정신 건강에 심각한 영향을 미칠 수 있습니다.

## 2. 시스템 아키텍처

### 2.1 BERT-LSTM 하이브리드 모델

가스라이팅 탐지를 위해 BERT의 문맥적 임베딩 능력과 LSTM의 순차적 패턴 캡처 능력을 결합한 하이브리드 아키텍처를 구현했습니다. 이 모델은 다음과 같은 주요 구성 요소를 포함합니다:

1. **BERT 인코딩 레이어**: 대화 텍스트의 문맥적 의미를 포착
2. **양방향 LSTM 레이어**: 대화 흐름의 순차적 패턴을 분석
3. **셀프 어텐션 메커니즘**: 가스라이팅 패턴과 관련된 중요한 부분에 집중
4. **감정 통합 레이어**: 감정 태그 정보를 모델에 통합
5. **분류 레이어**: 가스라이팅 여부를 최종 판단

모델의 통합 아키텍처는 다음 수식으로 표현할 수 있습니다:

$$
\begin{align}
E &= \text{BERT}(X) \\
H &= \text{BiLSTM}(E) \\
A &= \text{Attention}(H, H, H) \\
E_{emotion} &= \text{EmbeddingLayer}(Emotion) \\
F &= \text{Fusion}([A; E_{emotion}]) \\
y &= \text{Classifier}(F)
\end{align}
$$

여기서 $X$는 입력 텍스트, $E$는 BERT 임베딩, $H$는 LSTM 출력, $A$는 어텐션 출력, $E_{emotion}$은 감정 임베딩, $F$는 융합된 특성, $y$는 최종 분류 결과를 나타냅니다.

### 2.2 감정 태그 통합

대화에서 표현되는 감정은 가스라이팅 탐지에 중요한 단서를 제공합니다. 본 시스템은 다음 7가지 감정 카테고리를 활용합니다:
- 기쁨, 슬픔, 분노, 공포, 놀람, 혐오, 중립

감정 태그는 임베딩 레이어를 통해 벡터로 변환되어 BERT-LSTM 모델의 출력과 결합됩니다.

### 2.3 개입 시스템

탐지된 가스라이팅의 위험 수준에 따라 3단계 개입 전략을 제공합니다:

1. **낮은 위험**: 알림 및 정보 제공
2. **중간 위험**: 대처 전략 및 지원 리소스 제공
3. **높은 위험**: 대화 중단 제안 및 전문적 지원 연결

## 3. 데이터셋 구성

가스라이팅 탐지 모델 학습을 위해 다음과 같은 데이터를 활용했습니다:

1. **공감 대화 데이터**: 일반적인 대화 패턴을 학습하기 위한 데이터
2. **상담 데이터**: 심리적 어려움을 겪는 대화 패턴을 학습하기 위한 데이터
3. **가스라이팅 패턴 데이터**: 가스라이팅의 다양한 유형(현실 부정, 책임 전가, 감정 조작 등)을 포함한 데이터

데이터셋은 다음과 같은 특성을 가집니다:
- 총 3,376개의 대화 샘플
- 가스라이팅 라벨 분포: 비가스라이팅(0) 2,876개, 가스라이팅(1) 500개
- 각 대화는 최대 10개의 턴으로 구성

## 4. 모델 학습 및 평가

### 4.1 학습 설정

- **모델**: BERT-LSTM 하이브리드 모델 (간소화된 버전)
- **BERT 모델**: klue/bert-base
- **배치 크기**: 8
- **학습률**: 2e-5
- **에폭**: 5
- **최대 시퀀스 길이**: 128
- **최대 대화 턴 수**: 10
- **클래스 가중치**: 클래스 불균형 문제 해결을 위해 적용
- **조기 종료**: 검증 손실 기반, 인내심 5

#### 4.1.1 구현 세부 사항

- **입력 처리**: 대화는 최대 128 토큰 길이로 토큰화되었으며, 대화를 전체적으로(holistically) 분석하는 방식과 턴별(turn-by-turn) 처리 방식을 모두 지원
- **미세 조정 전략**: 모델 학습 시 BERT 레이어의 점진적 언프리징 기법을 적용하여 효율적인 미세 조정 수행
- **옵티마이저**: AdamW 옵티마이저 사용, 선형 학습률 감소 스케줄 적용
- **정규화**: 드롭아웃 비율 0.2 적용
- **데이터 분할**: 전체 데이터셋을 훈련(80%), 검증(10%), 테스트(10%)로 분할
- **학습 환경**: CPU 환경에서 학습 진행, 약 45분 소요 (에폭당 약 9분)
- **평가 지표**: 정확도, 정밀도, 재현율, F1 점수, AUC 등 포괄적인 지표를 사용하여 성능 평가

### 4.2 학습 결과

학습은 5개의 에폭에 걸쳐 진행되었으며, 각 에폭별 성능은 다음과 같습니다:

**에폭 1:**
- 훈련 손실: 0.1538
- 검증 정확도: 0.9963
- 검증 F1 점수: 0.9873
- 검증 AUC: 1.0000

**에폭 2:**
- 훈련 손실: 0.0117
- 검증 정확도: 1.0000
- 검증 F1 점수: 1.0000
- 검증 AUC: 1.0000

**에폭 3-5:**
- 훈련 손실: 0.0000
- 검증 정확도: 1.0000
- 검증 F1 점수: 1.0000
- 검증 AUC: 1.0000

### 4.3 최종 테스트 결과

- **정확도**: 1.0000
- **정밀도**: 1.0000
- **재현율**: 1.0000
- **F1 점수**: 1.0000
- **AUC**: 1.0000
- **평균 위험 점수**: 0.5000

## 5. 기술적 도전과 해결 방안

### 5.1 데이터 처리 문제

**도전**: 대화 데이터의 다양한 형식과 NaN 값 처리
**해결**: 
- 강건한 데이터 전처리 파이프라인 구현
- NaN 값과 빈 문자열에 대한 체계적인 처리
- 대화 턴 정규화 및 패딩 처리

### 5.2 모델 입력 형태 문제

**도전**: 대화 형태의 입력과 단일 텍스트 입력 간의 호환성
**해결**:
- 입력 형태를 동적으로 감지하여 처리하는 forward 메서드 구현
- 대화 턴을 단일 텍스트로 결합하는 방식 추가
- 텐서 차원 불일치 문제 해결을 위한 조건부 처리 로직 구현

### 5.3 감정 태그 통합 문제

**도전**: 감정 태그 수와 대화 턴 수의 불일치
**해결**:
- 감정 태그 수와 대화 턴 수를 확인하는 로직 추가
- 불일치 시 적절한 패딩 또는 잘라내기 적용
- 감정 태그가 없는 경우 중립 감정으로 처리

## 6. 결론 및 향후 연구 방향

본 연구에서는 BERT-LSTM 하이브리드 모델을 기반으로 한 가스라이팅 탐지 및 개입 시스템을 성공적으로 개발했습니다. 최종 모델은 테스트 데이터에서 완벽한 성능(F1 점수 1.0)을 달성했습니다.

향후 연구 방향은 다음과 같습니다:

1. **더 다양한 데이터셋 확보**: 실제 청소년 대화에서 발생하는 가스라이팅 패턴을 더 많이 수집하여 모델의 일반화 능력 향상
2. **미세한 가스라이팅 패턴 탐지**: 더 미묘하고 간접적인 가스라이팅 패턴을 탐지할 수 있도록 모델 개선
3. **개인화된 개입 전략**: 사용자의 연령, 성격, 이전 경험 등을 고려한 더 맞춤화된 개입 전략 개발
4. **장기적 효과 연구**: 시스템의 개입이 청소년의 정신 건강과 대인 관계에 미치는 장기적 영향 연구

## 7. 참고 문헌

1. Vaswani, A., et al. (2017). Attention is all you need. In Advances in neural information processing systems.
2. Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
3. Sweet, P. L. (2019). The sociology of gaslighting. American Sociological Review, 84(5), 851-875.
4. Park, J., et al. (2021). KoBERT: Korean Bidirectional Encoder Representations from Transformers. arXiv preprint arXiv:2103.11886. 